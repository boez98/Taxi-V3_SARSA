{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e7fe6b",
   "metadata": {},
   "source": [
    "## Taxi-V3 using SARSA made by Andrea Bolla - 4482930\n",
    "\n",
    "SARSA algorithm is a slight variation of the popular Q-Learning algorithm. For a learning agent in any Reinforcement Learning algorithm it’s policy can be of two types:\n",
    "\n",
    "    On Policy: the learning agent learns the value function according to the current action derived from the policy currently being used. \n",
    "    Off Policy: the learning agent learns the value function according to the action derived from another policy. \n",
    "\n",
    "Q-Learning technique is an Off Policy technique and uses the greedy approach to learn the Q-value. SARSA technique, on the other hand, is an On Policy and uses the action performed by the current policy to learn the Q-value.\n",
    "\n",
    "The equation for SARSA depends on the current state, current action, reward obtained, next state and next action. SARSA stands for State Action Reward State Action which symbolizes the tuple (s, a, r, s’, a’)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dbc69b",
   "metadata": {},
   "source": [
    "# Step 1: Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45333e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/boez/opt/anaconda3/lib/python3.9/site-packages')\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06840eed",
   "metadata": {},
   "source": [
    "sys: is used for the path of my libraries (NB change it with yours)\n",
    "\n",
    "gym: is a standard API for reinforcement learning, and has diverse collection of reference environments\n",
    "\n",
    "numpy: is a Python library used for working with arrays\n",
    "\n",
    "random:  is a module to implement pseudo-random number generators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f112d",
   "metadata": {},
   "source": [
    "# Step 2: Building the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c7089e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\").env\n",
    "\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07772222",
   "metadata": {},
   "source": [
    "env.reset: Resets the environment and returns a random initial state.\n",
    "\n",
    "env.step(action): Step the environment by one timestep. Returns:\n",
    "    \n",
    "    observation: Observations of the environment\n",
    "\n",
    "    reward: If your action was beneficial or not\n",
    "\n",
    "    done: Indicates if we have successfully picked up and dropped off a passenger, also called one episode\n",
    "\n",
    "    truncated: if episode truncates due to a time limit or a reason that is not defined as part of the task MDP.\n",
    "\n",
    "    info: Additional info such as performance and latency for debugging purposes\n",
    "\n",
    "env.render: Renders one frame of the environment from 0-5 where:\n",
    "\n",
    "    0 = south\n",
    "    1 = north\n",
    "    2 = east\n",
    "    3 = west\n",
    "    4 = pickup\n",
    "    5 = dropoff\n",
    "\n",
    "The 500 states correspond to a encoding of the taxi's location, the passenger's location, and the destination location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69d44f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 223, -1, False)],\n",
       " 1: [(1.0, 23, -1, False)],\n",
       " 2: [(1.0, 123, -1, False)],\n",
       " 3: [(1.0, 103, -1, False)],\n",
       " 4: [(1.0, 123, -10, False)],\n",
       " 5: [(1.0, 123, -10, False)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90fd084",
   "metadata": {},
   "source": [
    "The 0-5 corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at our current state in the illustration.\n",
    "\n",
    "In this env, probability is always 1.0.\n",
    "\n",
    "The nextstate is the state we would be in if we take the action at this index of the dict\n",
    "\n",
    "All the movement actions have a -1 reward \n",
    "the pickup/dropoff actions have -10 reward in this particular state. \n",
    "If we are in a state where the taxi has a passenger and is on top of the right destination, we would see a reward of 20 at the dropoff action \n",
    "\n",
    "done is used to tell us when we have successfully dropped off a passenger in the right location. \n",
    "\n",
    "Each successfull dropoff is the end of an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a6515",
   "metadata": {},
   "source": [
    "# Step 3: Initializing different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d05df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the different parameters\n",
    "epsilon = 1 # Total exploration and no exploitation\n",
    "alpha = 0.3\n",
    "gamma = 0.95\n",
    "\n",
    "# Training parameters\n",
    "n_episodes = 100000  # number of episodes to use for training\n",
    "n_max_steps = 100   # maximum number of steps per episode\n",
    "\n",
    "#Initializing the Q-table 500x6\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3070b52",
   "metadata": {},
   "source": [
    "The equation for SARSA depends on the current state, current action, reward obtained, next state and next action.\n",
    "\n",
    "Q(s_t,a_t) = Q(s_t,a_t) + alpha* ( r_(t+1) + gamma* Q(s_(t+1),a_(t+1)) - Q(s_t,a_t) )\n",
    "\n",
    "    \n",
    "Where: \n",
    "     \n",
    "    ϵ (epsilon) is the paramenter which choose between exploration (choosing a random action) and exploitation (choosing actions based on already learned Q-values). \n",
    "    \n",
    "    α (alpha) is the learning rate, it is the extent to which our Q-values are being updated in every iteration.\n",
    "    \n",
    "    γ (gamma) is the discount factor  determines how much importance we want to give to future rewards. A high value for the discount factor (close to 1) captures the long-term effective award, insted a discount factor of 0 makes our agent consider only immediate reward (greedy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969f8a3",
   "metadata": {},
   "source": [
    "# Step 4: Defining utility functions to be used in the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad342b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to choose the next action\n",
    "def choose_action(state):\n",
    "\n",
    "    action=0\n",
    "\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "\n",
    "        action = env.action_space.sample()   # explore\n",
    "\n",
    "    else:\n",
    "        \n",
    "        action = np.argmax(Q[state, :])      # exploit\n",
    "        \n",
    "    return action\n",
    "\n",
    "# Function to update the Q-value\n",
    "def update(state, state2, reward, action, action2):\n",
    "    \n",
    "    predict = Q[state, action]\n",
    "    target = reward + gamma * Q[state2, action2]\n",
    "    \n",
    "    Q[state, action] = Q[state, action] + alpha * (target - predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3613d1a7",
   "metadata": {},
   "source": [
    "Choose_action() allow the agent to choose the next action, it all depens on the random number and the epsilon value, if epsilon is bigger than the random number the agent will choose to explore, otherwise to exploit and choose in the update Q table the best action.\n",
    "\n",
    "Update() is used to update the Q table, following the SARSA equation, when the agent choose to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b17df",
   "metadata": {},
   "source": [
    "# Step 5: Training the learning agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625ee021",
   "metadata": {},
   "source": [
    "To train the agent we use the function see in the step 4. \n",
    "\n",
    "We start having a pure exploration to fill the Q table but with the episodes increse we want that the explotation increse too. To obtain that we are decresing epsilon every episode. After 200 episode (20000 step) we will be in a pure explotation state.\n",
    "\n",
    "\n",
    "Each episode require a different number of steps to complete the task, so we don’t always receive the same reward (each move is minus 1 point).\n",
    "\n",
    "The minimum reward will be 3 (20 - 17), it takes 17 steps (16 moves + 1 pick up action) if we initialised the taxi and passenger at opposite corners with a drop-off location being the same corner as the taxi’s original position. \n",
    "\n",
    "The maximum reward is 15, we cannot expect any higher, if we take the two closest colour squares (Red and Yellow), the agent would use 5 moves (1 to pick up + 4 to move)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e911b291",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d23efbc",
   "metadata": {},
   "source": [
    "The program should run for hundreds/thousands of episodes (many hours) to learn and have good result, so I will attach some results.\n",
    "\n",
    "At the beginning we have pure exploration, the agent will explore every situation making a lot of mistakes. We can see that in the firts 100 episode it reached the goal only 3 times.\n",
    "\n",
    "    Episode:  1\n",
    "    Goal reached:  0\n",
    "    Score:  -334\n",
    "\n",
    "    Episode:  100\n",
    "    Goal reached:  3\n",
    "    Score:  -307\n",
    "\n",
    "After 100 episodes we are in a 50/50 between exploration and explotation, due to the decreasing value of epsilon. Here we can see that the agent is still making mistakes but it start to reach some goals.\n",
    "\n",
    "    Episode:  201\n",
    "    Goal reached:  26\n",
    "    Score:  -100\n",
    "\n",
    "Now, after 200 episodes we are in pure explotation, the agent should have learned enough to take good decision, we see that in the last 100 episode (201 to 302) it reached the goal 74 times but it's not perfect yet.\n",
    "\n",
    "    Episode:  302\n",
    "    Goal reached:  100\n",
    "    Score:  -13\n",
    "\n",
    "Then, after 400 episodes the agent knows perfectly how to interact in every situation taking the best action and the shortest way to reach the goal. It's rare that he doesn't reach the goal. \n",
    "\n",
    "    Episode:  403\n",
    "    Goal reached:  193\n",
    "    Score:  8\n",
    "    \n",
    "    Episode:  3475\n",
    "    Goal reached:  3261\n",
    "    Score:  8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
